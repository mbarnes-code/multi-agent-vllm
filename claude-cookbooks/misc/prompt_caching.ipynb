{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt caching through the Claude API\n",
    "\n",
    "Prompt caching allows you to store and reuse context within your prompt. This makes it more practical to include additional information in your prompt—such as detailed instructions and example responses—which help improve every response Claude generates.\n",
    "\n",
    "In addition, by fully leveraging prompt caching within your prompt, you can reduce latency by >2x and costs up to 90%. This can generate significant savings when building solutions that involve repetitive tasks around detailed book_content.\n",
    "\n",
    "In this cookbook, we will demonstrate how to use prompt caching in a single turn and across a multi-turn conversation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up our environment with the necessary imports and initializations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install anthropic bs4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import anthropic\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "MODEL_NAME = \"claude-sonnet-4-5\"\n",
    "TIMESTAMP = int(time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fetch some text content to use in our examples. We'll use the text from Pride and Prejudice by Jane Austen which is around ~187,000 tokens long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_article_content(url):\n",
    "    response = requests.get(url, timeout=30)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Remove script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "\n",
    "    # Get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # Break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # Break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # Drop blank lines\n",
    "    text = \"\\n\".join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Fetch the content of the article\n",
    "book_url = \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\"\n",
    "book_content = fetch_article_content(book_url)\n",
    "\n",
    "print(f\"Fetched {len(book_content)} characters from the book.\")\n",
    "print(\"First 500 characters:\")\n",
    "print(book_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Single turn\n",
    "\n",
    "Let's demonstrate prompt caching with a large document, comparing the performance and cost between cached and non-cached API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Non-cached API Call (Baseline)\n",
    "\n",
    "First, let's make a truly non-cached API call **without** the `cache_control` parameter. This will establish our baseline performance.\n",
    "\n",
    "We'll ask for a short output to keep response generation time low, since prompt caching only affects input processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-cached API call time: 6.86 seconds\n",
      "Input tokens: 187363\n",
      "Output tokens: 8\n",
      "\n",
      "Response:\n",
      "Pride and Prejudice\n"
     ]
    }
   ],
   "source": [
    "def make_non_cached_api_call():\n",
    "    \"\"\"Make an API call WITHOUT cache_control - no caching enabled.\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": str(TIMESTAMP) + \"<book>\" + book_content + \"</book>\",\n",
    "                    # Note: No cache_control parameter here - this is truly non-cached\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": \"What is the title of this book? Only output the title.\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=300,\n",
    "        messages=messages,\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    return response, end_time - start_time\n",
    "\n",
    "\n",
    "non_cached_response, non_cached_time = make_non_cached_api_call()\n",
    "\n",
    "print(f\"Non-cached API call time: {non_cached_time:.2f} seconds\")\n",
    "print(f\"Input tokens: {non_cached_response.usage.input_tokens}\")\n",
    "print(f\"Output tokens: {non_cached_response.usage.output_tokens}\")\n",
    "\n",
    "print(\"\\nResponse:\")\n",
    "print(non_cached_response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: First Cached API Call (Cache Creation)\n",
    "\n",
    "Now let's enable prompt caching by adding `cache_control: {\"type\": \"ephemeral\"}` to the book content. \n",
    "\n",
    "**Important:** The first call with `cache_control` will **create** the cache entry. This initial call will have similar timing to the non-cached call because it still needs to process all tokens. However, it will store them in the cache for future use.\n",
    "\n",
    "Look for the `cache_creation_input_tokens` field in the usage stats to see how many tokens were cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First cached API call time: 5.96 seconds\n",
      "Input tokens: 16\n",
      "Output tokens: 8\n",
      "Cache creation tokens: 187347\n",
      "\n",
      "Response:\n",
      "Pride and Prejudice\n",
      "\n",
      "Note: This first call creates the cache but doesn't benefit from it yet - timing is similar to non-cached call.\n"
     ]
    }
   ],
   "source": [
    "def make_cached_api_call_create():\n",
    "    \"\"\"First call WITH cache_control - creates the cache entry.\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": str(TIMESTAMP) + \"<book>\" + book_content + \"</book>\",\n",
    "                    \"cache_control\": {\"type\": \"ephemeral\"},  # This enables caching\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": \"What is the title of this book? Only output the title.\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=300,\n",
    "        messages=messages,\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    return response, end_time - start_time\n",
    "\n",
    "\n",
    "cached_create_response, cached_create_time = make_cached_api_call_create()\n",
    "\n",
    "print(f\"First cached API call time: {cached_create_time:.2f} seconds\")\n",
    "print(f\"Input tokens: {cached_create_response.usage.input_tokens}\")\n",
    "print(f\"Output tokens: {cached_create_response.usage.output_tokens}\")\n",
    "print(\n",
    "    f\"Cache creation tokens: {getattr(cached_create_response.usage, 'cache_creation_input_tokens', 0)}\"\n",
    ")\n",
    "\n",
    "print(\"\\nResponse:\")\n",
    "print(cached_create_response.content[0].text)\n",
    "\n",
    "print(\n",
    "    \"\\nNote: This first call creates the cache but doesn't benefit from it yet - timing is similar to non-cached call.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Second Cached API Call (Cache Hit)\n",
    "\n",
    "Now let's make another API call with the same `cache_control` parameter. Since the cache was created in Part 2, this call will **read from the cache** instead of processing all tokens again.\n",
    "\n",
    "This is where you see the real performance benefit! Look for the `cache_read_input_tokens` field in the usage stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second cached API call time: 3.66 seconds\n",
      "Input tokens: 16\n",
      "Output tokens: 8\n",
      "Cache read tokens: 187347\n",
      "\n",
      "Response:\n",
      "Pride and Prejudice\n",
      "\n",
      "======================================================================\n",
      "PERFORMANCE COMPARISON\n",
      "======================================================================\n",
      "Non-cached call:       6.86s\n",
      "First cached call:     5.96s (creates cache)\n",
      "Second cached call:    3.66s (reads from cache)\n",
      "\n",
      "Speedup from caching:  1.9x faster!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def make_cached_api_call_hit():\n",
    "    \"\"\"Second call WITH cache_control - reads from existing cache.\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": str(TIMESTAMP) + \"<book>\" + book_content + \"</book>\",\n",
    "                    \"cache_control\": {\"type\": \"ephemeral\"},  # Same cache_control as before\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": \"What is the title of this book? Only output the title.\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=300,\n",
    "        messages=messages,\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    return response, end_time - start_time\n",
    "\n",
    "\n",
    "cached_hit_response, cached_hit_time = make_cached_api_call_hit()\n",
    "\n",
    "print(f\"Second cached API call time: {cached_hit_time:.2f} seconds\")\n",
    "print(f\"Input tokens: {cached_hit_response.usage.input_tokens}\")\n",
    "print(f\"Output tokens: {cached_hit_response.usage.output_tokens}\")\n",
    "print(f\"Cache read tokens: {getattr(cached_hit_response.usage, 'cache_read_input_tokens', 0)}\")\n",
    "\n",
    "print(\"\\nResponse:\")\n",
    "print(cached_hit_response.content[0].text)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Non-cached call:       {non_cached_time:.2f}s\")\n",
    "print(f\"First cached call:     {cached_create_time:.2f}s (creates cache)\")\n",
    "print(f\"Second cached call:    {cached_hit_time:.2f}s (reads from cache)\")\n",
    "print(f\"\\nSpeedup from caching:  {non_cached_time / cached_hit_time:.1f}x faster!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Example 1\n",
    "\n",
    "This example demonstrated three distinct scenarios:\n",
    "\n",
    "1. **Non-cached call** - Without `cache_control`, Claude processes all ~187k tokens normally\n",
    "2. **First cached call** - With `cache_control`, Claude processes all tokens AND stores them in cache (similar timing to non-cached)\n",
    "3. **Second cached call** - With `cache_control`, Claude reads from the existing cache (2-10x faster!)\n",
    "\n",
    "The key insight: **Prompt caching requires two calls to show benefits**\n",
    "- The first call with `cache_control` creates the cache entry\n",
    "- Subsequent calls with the same `cache_control` read from the cache for dramatic speedups\n",
    "\n",
    "This is especially valuable for:\n",
    "- Large documents or codebases that remain constant across multiple queries\n",
    "- System prompts with detailed instructions\n",
    "- Multi-turn conversations (as shown in Example 2 below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Multi-turn Conversation with Incremental Caching\n",
    "\n",
    "Now, let's look at a multi-turn conversation where we add cache breakpoints as the conversation progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Turn 1:\n",
      "User: What is the title of this novel?\n",
      "Assistant: The title of this novel is **Pride and Prejudice** by Jane Austen.\n",
      "User input tokens: 3\n",
      "Output tokens: 22\n",
      "Input tokens (cache read): 0\n",
      "Input tokens (cache write): 187360\n",
      "0.0% of input prompt cached (3 tokens)\n",
      "Time taken: 5.79 seconds\n",
      "\n",
      "Turn 2:\n",
      "User: Who are Mr. and Mrs. Bennet?\n",
      "Assistant: Mr. and Mrs. Bennet are the parents of five daughters: Jane, Elizabeth, Mary, Kitty, and Lydia. \n",
      "\n",
      "**Mr. Bennet** is described as an intelligent, sarcastic man with \"quick parts, sarcastic humour, reserve, and caprice.\" He tends to be detached and ironic, often amusing himself at his wife's expense, and prefers to spend time in his library rather than deal with family matters.\n",
      "\n",
      "**Mrs. Bennet** is described as \"a woman of mean understanding, little information, and uncertain temper.\" She is nervous, excitable, and foolish, with her main goal in life being to get her daughters married. She lacks the intelligence and social graces of her husband and is often oblivious to her own impropriety.\n",
      "\n",
      "Their contrasting personalities create much of the domestic tension and comedy in the novel. Mr. Bennet married Mrs. Bennet when he was \"captivated by youth and beauty,\" but her weak understanding soon ended any real affection he had for her, leaving him to cope with his disappointment through ironic detachment.\n",
      "User input tokens: 3\n",
      "Output tokens: 247\n",
      "Input tokens (cache read): 187360\n",
      "Input tokens (cache write): 36\n",
      "100.0% of input prompt cached (187363 tokens)\n",
      "Time taken: 8.29 seconds\n",
      "\n",
      "Turn 3:\n",
      "User: What is Netherfield Park?\n",
      "Assistant: **Netherfield Park** is a large estate in Hertfordshire that is rented by Mr. Bingley at the beginning of the novel. \n",
      "\n",
      "It is located about three miles from Longbourn, the Bennet family's home, making it conveniently close for social visits. The estate becomes the center of much excitement and speculation when Mr. Bingley, a wealthy young bachelor, takes up residence there.\n",
      "\n",
      "Key points about Netherfield:\n",
      "\n",
      "- It's described as a good house with pleasant grounds\n",
      "- Mr. Bingley rents it (rather than owning it), as he has not yet purchased an estate of his own\n",
      "- His sisters, Caroline Bingley and Mrs. Hurst, live with him there, along with Mrs. Hurst's husband\n",
      "- Mr. Darcy, Bingley's close friend, is a frequent visitor\n",
      "- The famous ball where Elizabeth and Darcy have their first significant interactions takes place at Netherfield\n",
      "- Jane Bennet stays there when she falls ill after riding over in the rain, which allows Elizabeth to visit and spend time at the estate\n",
      "\n",
      "Netherfield Park is important to the plot as it brings the wealthy Mr. Bingley (and Mr. Darcy) into the neighborhood, setting the main romantic storylines in motion.\n",
      "User input tokens: 3\n",
      "Output tokens: 293\n",
      "Input tokens (cache read): 187396\n",
      "Input tokens (cache write): 258\n",
      "100.0% of input prompt cached (187399 tokens)\n",
      "Time taken: 10.14 seconds\n",
      "\n",
      "Turn 4:\n",
      "User: What is the main theme of this novel?\n",
      "Assistant: The main themes of **Pride and Prejudice** include:\n",
      "\n",
      "**1. Pride and Prejudice (as the title suggests)**\n",
      "- The novel explores how pride and prejudice create misunderstandings and obstacles to happiness\n",
      "- **Darcy's pride** in his social status initially leads him to insult Elizabeth and look down on her family\n",
      "- **Elizabeth's prejudice** against Darcy (based on first impressions and Wickham's lies) blinds her to his true character\n",
      "- Both must overcome these flaws to find happiness together\n",
      "\n",
      "**2. Class and Social Status**\n",
      "- The rigid class distinctions of Regency England and their impact on relationships and marriage prospects\n",
      "- The tension between wealth, birth, and merit as measures of a person's worth\n",
      "- Lady Catherine's objections to Elizabeth based on her \"inferior\" connections\n",
      "\n",
      "**3. Marriage and Economics**\n",
      "- The novel examines different motivations for marriage: love (Jane and Bingley), practicality (Charlotte and Mr. Collins), lust and recklessness (Lydia and Wickham), and the ideal combination of respect, affection, and compatibility (Elizabeth and Darcy)\n",
      "- The economic pressures on women to marry well, especially with the entailment of the Bennet estate\n",
      "\n",
      "**4. Self-Knowledge and Personal Growth**\n",
      "- Elizabeth's journey from prejudice to understanding\n",
      "User input tokens: 3\n",
      "Output tokens: 300\n",
      "Input tokens (cache read): 187654\n",
      "Input tokens (cache write): 305\n",
      "100.0% of input prompt cached (187657 tokens)\n",
      "Time taken: 9.53 seconds\n"
     ]
    }
   ],
   "source": [
    "class ConversationHistory:\n",
    "    def __init__(self):\n",
    "        # Initialize an empty list to store conversation turns\n",
    "        self.turns = []\n",
    "\n",
    "    def add_turn_assistant(self, content):\n",
    "        # Add an assistant's turn to the conversation history\n",
    "        self.turns.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": content}]})\n",
    "\n",
    "    def add_turn_user(self, content):\n",
    "        # Add a user's turn to the conversation history\n",
    "        self.turns.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": content}]})\n",
    "\n",
    "    def get_turns(self):\n",
    "        # Retrieve conversation turns with specific formatting\n",
    "        result = []\n",
    "        user_turns_processed = 0\n",
    "        # Iterate through turns in reverse order\n",
    "        for turn in reversed(self.turns):\n",
    "            if turn[\"role\"] == \"user\" and user_turns_processed < 1:\n",
    "                # Add the last user turn with ephemeral cache control\n",
    "                result.append(\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": turn[\"content\"][0][\"text\"],\n",
    "                                \"cache_control\": {\"type\": \"ephemeral\"},\n",
    "                            }\n",
    "                        ],\n",
    "                    }\n",
    "                )\n",
    "                user_turns_processed += 1\n",
    "            else:\n",
    "                # Add other turns as they are\n",
    "                result.append(turn)\n",
    "        # Return the turns in the original order\n",
    "        return list(reversed(result))\n",
    "\n",
    "\n",
    "# Initialize the conversation history\n",
    "conversation_history = ConversationHistory()\n",
    "\n",
    "# System message containing the book content\n",
    "# Note: 'book_content' should be defined elsewhere in the code\n",
    "system_message = f\"{TIMESTAMP} <file_contents> {book_content} </file_contents>\"\n",
    "\n",
    "# Predefined questions for our simulation\n",
    "questions = [\n",
    "    \"What is the title of this novel?\",\n",
    "    \"Who are Mr. and Mrs. Bennet?\",\n",
    "    \"What is Netherfield Park?\",\n",
    "    \"What is the main theme of this novel?\",\n",
    "]\n",
    "\n",
    "\n",
    "def simulate_conversation():\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\nTurn {i}:\")\n",
    "        print(f\"User: {question}\")\n",
    "\n",
    "        # Add user input to conversation history\n",
    "        conversation_history.add_turn_user(question)\n",
    "\n",
    "        # Record the start time for performance measurement\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Make an API call to the assistant\n",
    "        response = client.messages.create(\n",
    "            model=MODEL_NAME,\n",
    "            max_tokens=300,\n",
    "            system=[\n",
    "                {\"type\": \"text\", \"text\": system_message, \"cache_control\": {\"type\": \"ephemeral\"}},\n",
    "            ],\n",
    "            messages=conversation_history.get_turns(),\n",
    "        )\n",
    "\n",
    "        # Record the end time\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Extract the assistant's reply\n",
    "        assistant_reply = response.content[0].text\n",
    "        print(f\"Assistant: {assistant_reply}\")\n",
    "\n",
    "        # Print token usage information\n",
    "        input_tokens = response.usage.input_tokens\n",
    "        output_tokens = response.usage.output_tokens\n",
    "        input_tokens_cache_read = getattr(response.usage, \"cache_read_input_tokens\", \"---\")\n",
    "        input_tokens_cache_create = getattr(response.usage, \"cache_creation_input_tokens\", \"---\")\n",
    "        print(f\"User input tokens: {input_tokens}\")\n",
    "        print(f\"Output tokens: {output_tokens}\")\n",
    "        print(f\"Input tokens (cache read): {input_tokens_cache_read}\")\n",
    "        print(f\"Input tokens (cache write): {input_tokens_cache_create}\")\n",
    "\n",
    "        # Calculate and print the elapsed time\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        # Calculate the percentage of input prompt cached\n",
    "        total_input_tokens = input_tokens + (\n",
    "            int(input_tokens_cache_read) if input_tokens_cache_read != \"---\" else 0\n",
    "        )\n",
    "        percentage_cached = (\n",
    "            int(input_tokens_cache_read) / total_input_tokens * 100\n",
    "            if input_tokens_cache_read != \"---\" and total_input_tokens > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        print(f\"{percentage_cached:.1f}% of input prompt cached ({total_input_tokens} tokens)\")\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        # Add assistant's reply to conversation history\n",
    "        conversation_history.add_turn_assistant(assistant_reply)\n",
    "\n",
    "\n",
    "# Run the simulated conversation\n",
    "simulate_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in this example, response times decreased from nearly 24 seconds to just 7-11 seconds after the initial cache setup, while maintaining the same level of quality across the answers. Most of this remaining latency is due to the time it takes to generate the response, which is not affected by prompt caching.\n",
    "\n",
    "And since nearly 100% of input tokens were cached in subsequent turns as we kept adjusting the cache breakpoints, we were able to read the next user message nearly instantly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anthropic-cookbook (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
