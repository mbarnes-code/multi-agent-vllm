apiVersion: apps/v1
kind: Deployment
metadata:
  name: image-gen
  namespace: image-gen
  labels:
    app.kubernetes.io/name: image-gen
    app.kubernetes.io/component: inference
spec:
  replicas: 2  # One per node for load balancing
  selector:
    matchLabels:
      app: image-gen
  template:
    metadata:
      labels:
        app: image-gen
        app.kubernetes.io/name: image-gen
    spec:
      # Anti-affinity to spread across nodes
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: image-gen
              topologyKey: kubernetes.io/hostname
      
      # Tolerate GPU nodes
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      
      # Use NVIDIA runtime for GPU access
      runtimeClassName: nvidia
      
      containers:
        - name: image-gen
          image: nvcr.io/nvidia/pytorch:25.04-py3
          imagePullPolicy: IfNotPresent
          
          command:
            - /bin/bash
            - -c
            - |
              set -e
              echo "Installing dependencies..."
              pip install -q diffusers transformers accelerate gradio uvicorn fastapi sentencepiece protobuf
              
              echo "Starting image generation server..."
              cd /app
              python server.py --model ${MODEL_NAME:-qwen-image-2512} --port 7860
          
          ports:
            - name: http
              containerPort: 7860
              protocol: TCP
          
          env:
            - name: MODEL_NAME
              value: "qwen-image-2512"  # Override via ConfigMap
            - name: HF_HOME
              value: "/models/huggingface"
            - name: IMAGE_STORAGE_DIR
              value: "/generated"  # NFS-backed shared storage
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
                  optional: true
            - name: PYTORCH_CUDA_ALLOC_CONF
              value: "max_split_size_mb:512"
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          
          resources:
            requests:
              memory: "32Gi"
              cpu: "4"
              nvidia.com/gpu: "1"
            limits:
              memory: "64Gi"
              cpu: "8"
              nvidia.com/gpu: "1"
          
          volumeMounts:
            - name: app-code
              mountPath: /app
            - name: model-cache
              mountPath: /models
            - name: generated-images
              mountPath: /generated
            - name: shm
              mountPath: /dev/shm
          
          livenessProbe:
            httpGet:
              path: /api/health
              port: 7860
            initialDelaySeconds: 600  # Models take time to load (10+ min for large models)
            periodSeconds: 60
            timeoutSeconds: 30
            failureThreshold: 5
          
          readinessProbe:
            httpGet:
              path: /api/health
              port: 7860
            initialDelaySeconds: 120  # Reduced - model is cached on hostPath
            periodSeconds: 15
            timeoutSeconds: 10
            failureThreshold: 20
      
      volumes:
        - name: app-code
          configMap:
            name: image-gen-server
            defaultMode: 0755
        - name: model-cache
          hostPath:
            path: /data/models/image-gen
            type: DirectoryOrCreate
        - name: generated-images
          nfs:
            server: 10.10.10.1
            path: /nfs/imagegen
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: "16Gi"
      
      terminationGracePeriodSeconds: 30
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: image-gen-config
  namespace: image-gen
data:
  MODEL_NAME: "qwen-image-2512"
