# Service for Distributed vLLM (KubeRay) inference server
#
# Exposes the OpenAI-compatible API endpoint from the Ray head node.
# Uses MetalLB LoadBalancer for external access from LAN.
#
# API Endpoints (OpenAI-compatible):
#   - POST /v1/chat/completions    - Chat completions
#   - POST /v1/completions         - Text completions
#   - GET  /v1/models              - List available models
#   - GET  /health                 - Health check
#
apiVersion: v1
kind: Service
metadata:
  name: vllm-distributed-service
  namespace: llm-inference
  labels:
    app.kubernetes.io/name: vllm-distributed
    app.kubernetes.io/component: api-gateway
spec:
  selector:
    ray.io/cluster: vllm-cluster
    ray.io/node-type: head
  ports:
    - name: vllm-api
      port: 8081
      targetPort: 8081
      protocol: TCP
    - name: ray-dashboard
      port: 8265
      targetPort: 8265
      protocol: TCP
  type: LoadBalancer
  # MetalLB will assign an IP from the pool (192.168.86.200-220)
  loadBalancerIP: 192.168.86.203

